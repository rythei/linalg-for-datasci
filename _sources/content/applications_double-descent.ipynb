{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3533514a",
   "metadata": {},
   "source": [
    "# The \"double descent\" phenomenon\n",
    "\n",
    "Let's suppose that we have samples $(\\boldsymbol{x}_1,y_1),\\dots, (\\boldsymbol{x}_n, y_n)$ generated via\n",
    "\n",
    "$$\n",
    "y_i = \\boldsymbol{x}_i^\\top \\boldsymbol{b}_\\star + \\varepsilon_i\n",
    "$$\n",
    "\n",
    "where $\\varepsilon_i$ is some noise. One interesting question from the statistical perspective is: how many samples $n$ we need to approximately recover $\\boldsymbol{b}_\\star$? For formally, suppose we fix $p$ as some reasonably large dimension. Given $n$ training samples, we fit a $\\hat{\\boldsymbol{b}}$ (either using $\\hat{\\boldsymbol{b}} = \\boldsymbol{X}^\\top (\\boldsymbol{XX}^\\top)^{-1}\\boldsymbol{y}$ or $\\hat{\\boldsymbol{b}} = (\\boldsymbol{X^\\top X})^{-1}\\boldsymbol{X}^\\top \\boldsymbol{y}$), and measure the error $e_n = \\|\\hat{\\boldsymbol{b}} - \\boldsymbol{b}_\\star\\|_2^2$. Intuitively, we might think that as $n$ get's bigger, $e_n$ will get smaller, since we're \"learning more about $\\boldsymbol{b}_\\star$\". Intruigingly, this is not always the case.\n",
    "\n",
    "Below we run a simple simulation to find $e_n$ as a function of $n$. It turns out that the shape of this plot depends largely on the \"signal-to-noise\" ratio, which is $\\|\\boldsymbol{\\beta}_\\star\\|_2 / \\sigma$ where $\\sigma^2 = \\text{Var}(\\varepsilon)$. Intuitively, if the signal-to-noise ratio is large, the \"signal\" term $\\boldsymbol{Xb}_\\star$ is larger than the noise part $\\varepsilon$.\n",
    "\n",
    "In this simulation, for various values of $n$, we draw a data matrix $\\boldsymbol{X} \\sim N(0,\\boldsymbol{I}) \\in \\mathbb{R}^{n\\times p}$ and compute $\\boldsymbol{y} = \\boldsymbol{Xb}_{\\star} + \\varepsilon$ where $\\|\\boldsymbol{b}_\\star\\|_2 = \\text{SNR}$ and $\\text{Var}(\\varepsilon) = 1$. We then fit $\\hat{\\boldsymbol{b}} = \\boldsymbol{X}^\\dagger \\boldsymbol{y}$ (where either $\\boldsymbol{X}^\\dagger = \\boldsymbol{X}^\\top (\\boldsymbol{XX}^\\top)^{-1}$ or $(\\boldsymbol{X^\\top X})^{-1}\\boldsymbol{X}^\\top$) and measure $e_n = \\|\\hat{\\boldsymbol{b}} - \\boldsymbol{b}_\\star\\|_2$. We repeat this for $T$ trials, and take the average error over these trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ee29803",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-96383dc5bdc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mN_TRIALS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0msimulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mb_star\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mb_star\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_star\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msnr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N_TRIALS = 100\n",
    "\n",
    "def simulation(p, n_min, n_max, snr=np.sqrt(5)):\n",
    "    b_star = np.random.normal(size=p)\n",
    "    b_star /= np.linalg.norm(b_star)/snr\n",
    "    errors = []\n",
    "    std = []\n",
    "    nn = [i for i in np.arange(n_min, n_max+1, 1) if i != p]\n",
    "    for n in nn:\n",
    "        temp = []\n",
    "        for t in range(N_TRIALS):\n",
    "\n",
    "            #generate data\n",
    "            X = np.random.normal(size=(n,p))\n",
    "            epsilon = np.random.normal(size=n)\n",
    "            y = np.dot(X, b_star) + epsilon\n",
    "\n",
    "            b_hat = np.dot(np.linalg.pinv(X),y) # find least squares solution\n",
    "\n",
    "            error = np.linalg.norm(b_hat-b_star)**2 # record error\n",
    "            temp.append(error)\n",
    "        errors.append(np.mean(temp))\n",
    "        std.append(np.std(temp))\n",
    "\n",
    "    return np.array(errors), np.array(std)\n",
    "\n",
    "\n",
    "\n",
    "p = 200\n",
    "n_min = 100\n",
    "n_max = 300\n",
    "\n",
    "\n",
    "errors, std = simulation(p=p, n_min=n_min, n_max=n_max)\n",
    "\n",
    "nn = [i for i in np.arange(n_min, n_max+1, 1) if i != p]\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.plot(nn, errors)\n",
    "plt.xlabel('n', fontsize=16)\n",
    "plt.ylabel('Error', fontsize=16)\n",
    "plt.title('Errors as a function of the number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982d0984",
   "metadata": {},
   "source": [
    "Somewhat amazingly, the plot of $e_n$ verses in $n$ is not monotone at all -- in fact, it reaches a maximum at $n=p$! So more data points do not always help us; this is particularly true when $p$ is very large. This phenomenon has recently been referred to as \"double descent\", and it is hypothesized that a similar phenomenon is behind the remarkable recent successes of neural networks."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.12,
    "jupytext_version": "1.9.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "source_map": [
   12,
   28,
   74
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}