{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cc47207",
   "metadata": {},
   "source": [
    "# Homework 12 Computational Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aab3d1",
   "metadata": {},
   "source": [
    "## Linear Regression on Boston Housing data\n",
    "\n",
    "Let's apply what we've learned about linear regression to an actual dataset, the Boston housing dataset (see https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html for a description), which is loaded below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4131bc58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rythei/opt/miniconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without changing anything.\n",
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "\n",
    "boston_dataset = load_boston()\n",
    "\n",
    "X = boston_dataset.data\n",
    "y = boston_dataset.target\n",
    "X = np.delete(X, [11], axis=1)\n",
    "n,p = X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af97880",
   "metadata": {},
   "source": [
    "As we see, this dataset has $p=12$ predictor variables, and $n=506$ samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c564e97",
   "metadata": {},
   "source": [
    "### Part 1 Adding a column of ones\n",
    "\n",
    "In the cell below, append a column of 1's to the matrix $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b51595b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.append(X, np.ones(X.shape[0]).reshape(-1,1), 1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced5aeea",
   "metadata": {},
   "source": [
    "### Part 2 Computing the least squares coefficients\n",
    "\n",
    "Recall the to solution to the least-squares problem is given by:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{b}} = (\\boldsymbol{X^\\top X})^{-1}\\boldsymbol{X^\\top y} = \\text{argmin}_{\\boldsymbol{b}} \\|\\boldsymbol{Xb} - \\boldsymbol{y}\\|_2^2 \n",
    "$$\n",
    "\n",
    "\n",
    "Compute the least squares coefficients for the data housing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9125a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_hat = # YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611f5b8b",
   "metadata": {},
   "source": [
    "### Part 3 Plotting residuals\n",
    "\n",
    "Next, we look at one important way to visualize the the quality of the fit from least squares using the _residuals_. The residuals are defined as the difference between the predicted $y$ values and the true $y$ values:\n",
    "\n",
    "$$\n",
    "\\mathbf{e} = \\mathbf{y} - \\hat{\\mathbf{y}} = \\mathbf{y} - \\mathbf{X}\\hat{\\mathbf{b}}\n",
    "$$\n",
    "\n",
    "In the cell below, fill in the function ```compute_residuals``` to compute this vector of residuals for a given data matrix $\\mathbf{X}$, response vector $\\mathbf{y}$ and coefficient vector $\\hat{\\mathbf{b}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c2bf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_residuals(X, y, b):\n",
    "    '''\n",
    "    Arguments:\n",
    "        X: n x (p+1) array of predictor variables\n",
    "        y: length n array of respones\n",
    "        b: length (p+1) array of regression coefficients\n",
    "    Returns:\n",
    "        array e containing residuals y - y_hat\n",
    "    '''\n",
    "    # YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a09ec78",
   "metadata": {},
   "source": [
    "Next, let's use this function to plot the residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2528abc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residuals(X, y, b):\n",
    "    residuals = compute_residuals(X, y, b)\n",
    "    fitted_values = np.dot(X,b)\n",
    "    plt.scatter(fitted_values, residuals)\n",
    "    plt.xticks(())\n",
    "    plt.xlabel('Fitted values', fontsize=16)\n",
    "    plt.ylabel('Residuals', fontsize=16)\n",
    "    plt.show()\n",
    "    \n",
    "plot_residuals(X, y, b_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9037a7",
   "metadata": {},
   "source": [
    "To interpret the residual plot, let's first recall our regression model:\n",
    "\n",
    "$$\n",
    "y = b_0 + b_1 X_1 + b_2 X_2 + \\cdots +  b_p X_p +\\varepsilon\n",
    "$$\n",
    "\n",
    "where we assume $\\varepsilon \\sim N(0,\\sigma^2)$ is a noise variable. If we plug in the fitted coefficients $\\hat{b}_i$ and subtract the fitted values from each side, we get that\n",
    "\n",
    "$$\n",
    "y - (\\hat{b}_0 + \\hat{b}_1 X_1 + \\hat{b}_2 X_2 + \\cdots +  \\hat{b}_p X_p) = \\varepsilon\n",
    "$$\n",
    "\n",
    "Thus, if our model is correct, we expect the residuals to be roughly normaly with mean zero. As we see from the plot above, the residuals do seem to be centered around zero. However, it's not clear from this plot alone that they actually follow a normal distribution. Let's look at the histogram of the residuals to get a slightly better picture of their distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4aebd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = compute_residuals(X, y, b_hat)\n",
    "plt.hist(residuals, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be836c8",
   "metadata": {},
   "source": [
    "From this plot, we see that the residuals do roughly fit a bell curve shape of a normal distribution, but it's not quite clear how well. In the next section, we investigate a more refined way to assess how well the residuals follow a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09258e47",
   "metadata": {},
   "source": [
    "### Part 4 QQ plots\n",
    "\n",
    "A QQ (quantile-quantile) plot is a popular way to assess how well two distributions match. If you're familiar with some probability, you can read more about QQ plots on the [Wikipedia page](https://en.wikipedia.org/wiki/Qâ€“Q_plot). For now, it will suffice to know that it's a convenient way to visualize how well two distributions match.\n",
    "\n",
    "In our case, we want to see how well the residuals follow a $N(0,\\sigma^2)$ distribution. To do this, we first need to know the variance $\\sigma^2$. We can estimate this from our residuals using the following formula:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{n-2}(\\mathbf{y}-\\hat{\\mathbf{y}})^\\top(\\mathbf{y} - \\hat{\\mathbf{y}}) = \\frac{1}{n-2}(\\mathbf{y}-\\mathbf{X}\\hat{\\mathbf{b}})^\\top(\\mathbf{y} - \\mathbf{X}\\hat{\\mathbf{b}})\n",
    "$$\n",
    "\n",
    "Fill in the function ```estimate_residual_variance``` below to compute $\\hat{\\sigma}^2$ below (hint: you can use your ```compute_residuals``` function you defined above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e76486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_residual_variance(X, y, b):\n",
    "    '''\n",
    "    Arguments:\n",
    "        X: n x (p+1) array of predictor variables\n",
    "        y: length n vector of respones\n",
    "        b: length (p+1) vector of regression coefficients\n",
    "        \n",
    "    Returns:\n",
    "        float sigma2, which is an estimate of the variance of epsilon in the regression model\n",
    "    '''\n",
    "    # YOUR CODE GOES HERE\n",
    "    # YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcf7c3c",
   "metadata": {},
   "source": [
    "The QQ plot compares two distributions by measuring how much their _quantiles_ align. We do this by plotting the quantiles of the residual distribution versus the quantiles of a normal distribution. Ideally, these would match exactly, and we would have that the quantiles lie on exactly on the line $y=x$. The following function uses your ```estimate_residual_variance``` and ```compute_residuals``` functions to generate a QQ plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f479ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def residual_quantile(q, residuals):\n",
    "    return np.quantile(residuals, q)\n",
    "\n",
    "def QQ_plot(X, y, b):\n",
    "    residuals = compute_residuals(X, y, b)\n",
    "    sigma2 = estimate_residual_variance(X, y, b)\n",
    "    q_range = np.arange(0.01,1,.01)\n",
    "    qq_data = []\n",
    "    for q in q_range:\n",
    "        res_q = residual_quantile(q, residuals)\n",
    "        norm_q = norm.ppf(q, scale=np.sqrt(sigma2))\n",
    "        qq_data.append([norm_q, res_q])\n",
    "    \n",
    "    qq_data = np.array(qq_data)\n",
    "    x_lb = np.min(qq_data)\n",
    "    x_ub = np.max(qq_data)\n",
    "    xran = np.arange(x_lb,x_ub)\n",
    "    plt.plot(qq_data[:,0], qq_data[:,1])\n",
    "    plt.plot(xran, xran, color='black', linestyle='--', linewidth=.5, label='y=x')\n",
    "    plt.legend()\n",
    "    plt.ylabel('Residual quantiles', fontsize=16)\n",
    "    plt.xlabel('Normal quantiles', fontsize=16)\n",
    "    plt.title('QQ plot', fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "QQ_plot(X, y, b_hat)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41a2562",
   "metadata": {},
   "source": [
    "As we can see, the QQ plot shows that the quantiles of the two distributions don't quite align (i.e., the quantiles do not lie near the line $y=x$). As we will see in the next problem, we can sometimes obtain a better fit by transforming the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6e3352",
   "metadata": {},
   "source": [
    "### Part 5 Regression with transformed data\n",
    "\n",
    "One method that is sometimes used to obtain a better fit in linear regression is to transform either the predictor variables or the response variable. Use the functions you've written in parts 1-4 to fit another regression model, except using $\\log(y)$ as the response rather than $y$. Plot the QQ function of the new regression model. How does it compare the one we obtain in Part 4?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529d36c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06c462f2",
   "metadata": {},
   "source": [
    "### Part 6 Confidence intervals for the regression coefficients\n",
    "\n",
    "Given a regression model \n",
    "\n",
    "$$\n",
    "y = \\boldsymbol{b}^\\top \\boldsymbol{x} + \\varepsilon\n",
    "$$\n",
    "\n",
    "with $\\text{Var}(\\varepsilon) = \\sigma^2$ and the least squares estimate $\\hat{\\boldsymbol{\\beta}}$ for the coefficients, we have a closed formula for the covariance matrix of the coefficients $\\hat{\\boldsymbol{\\beta}}$:\n",
    "\n",
    "$$\n",
    "\\text{Cov}(\\hat{\\boldsymbol{b}}) = \\sigma^2 (\\boldsymbol{X^\\top X})^{-1} \\approx \\hat{\\sigma}^2 (\\boldsymbol{X^\\top X})^{-1} = \\hat{\\boldsymbol{C}}\n",
    "$$\n",
    "\n",
    "where $\\hat{\\sigma}^2$ is the estimator defined in Part 4. This formula is extremely convenient because it allows us to form approximate 95% confidence intervals for the coefficients $\\hat{\\boldsymbol{b}}$ as follows:  \n",
    "\n",
    "$$\n",
    "\\text{CI}_{0.95}(\\hat{b}_j) = [\\hat{b}_j - 2\\sqrt{\\boldsymbol{C}_{jj}}, \\hat{b}_j + 2\\sqrt{\\boldsymbol{C}_{jj}}]\n",
    "$$\n",
    "\n",
    "Construct these confidence intervals for the each of the coefficients found in Part 5, and report which features' confidence intervals _do not_ contain the value 0 (meaning that with probability >= 0.95, these coefficients are greater different from zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aec68ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
