{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c0e6a66",
   "metadata": {},
   "source": [
    "# The \"double descent\" phenomenon\n",
    "\n",
    "Let's suppose that we have samples $(\\boldsymbol{x}_1,y_1),\\dots, (\\boldsymbol{x}_n, y_n)$ generated via\n",
    "\n",
    "$$\n",
    "y_i = \\boldsymbol{x}_i^\\top \\boldsymbol{b}_\\star + \\varepsilon_i\n",
    "$$\n",
    "\n",
    "where $\\varepsilon_i$ is noise, usually assumed to have mean zero and variance $\\sigma^2$, and $\\boldsymbol{b}_\\star$ is the unobserved \"ground truth\" which we would like to be able to estimate.\n",
    "\n",
    "One interesting question from the statistical perspective is: how many samples $n$ we need to approximately recover $\\boldsymbol{b}_\\star$? For formally, suppose we fix $p$ as some reasonably large dimension. Given $n$ training samples, we fit a $\\hat{\\boldsymbol{b}}$ (either using $\\hat{\\boldsymbol{b}} = \\boldsymbol{X}^\\top (\\boldsymbol{XX}^\\top)^{-1}\\boldsymbol{y}$ or $\\hat{\\boldsymbol{b}} = (\\boldsymbol{X^\\top X})^{-1}\\boldsymbol{X}^\\top \\boldsymbol{y}$), and measure the error $e_n = \\|\\hat{\\boldsymbol{b}} - \\boldsymbol{b}_\\star\\|_2^2$. Intuitively, we might think that as $n$ get's bigger, $e_n$ will get smaller, since we're \"learning more about $\\boldsymbol{b}_\\star$\". Intruigingly, this is not always the case.\n",
    "\n",
    "Below we run a simple simulation to find $e_n$ as a function of $n$. It turns out that the shape of this plot depends largely on the \"signal-to-noise\" ratio, which is $\\|\\boldsymbol{\\beta}_\\star\\|_2 / \\sigma$ where $\\sigma^2 = \\text{Var}(\\varepsilon)$. Intuitively, if the signal-to-noise ratio is large, the \"signal\" term $\\boldsymbol{Xb}_\\star$ is larger than the noise part $\\varepsilon$.\n",
    "\n",
    "In this simulation, for various values of $n$, we draw a data matrix $\\boldsymbol{X} \\sim N(0,\\boldsymbol{I}) \\in \\mathbb{R}^{n\\times p}$ and compute $\\boldsymbol{y} = \\boldsymbol{Xb}_{\\star} + \\varepsilon$ where $\\|\\boldsymbol{b}_\\star\\|_2 = \\text{SNR}$ and $\\text{Var}(\\varepsilon) = 1$. We then fit $\\hat{\\boldsymbol{b}} = \\boldsymbol{X}^\\dagger \\boldsymbol{y}$ (where either $\\boldsymbol{X}^\\dagger = \\boldsymbol{X}^\\top (\\boldsymbol{XX}^\\top)^{-1}$ or $(\\boldsymbol{X^\\top X})^{-1}\\boldsymbol{X}^\\top$) and measure $e_n = \\|\\hat{\\boldsymbol{b}} - \\boldsymbol{b}_\\star\\|_2$. We repeat this for $T$ trials, and take the average error over these trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49e73686",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0c26277e7d3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_min\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_max\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-0c26277e7d3a>\u001b[0m in \u001b[0;36msimulation\u001b[0;34m(p, n_min, n_max, snr)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_star\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mb_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpinv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# find least squares solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_hat\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mb_star\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;31m# record error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mpinv\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36mpinv\u001b[0;34m(a, rcond, hermitian)\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1989\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconjugate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1990\u001b[0;31m     \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhermitian\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhermitian\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1992\u001b[0m     \u001b[0;31m# discard small singular values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m         \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->DdD'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->ddd'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1648\u001b[0;31m         \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1649\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_realType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "N_TRIALS = 100\n",
    "\n",
    "def simulation(p, n_min, n_max, snr=np.sqrt(5)):\n",
    "    b_star = np.random.normal(size=p)\n",
    "    b_star /= np.linalg.norm(b_star)/snr\n",
    "    errors = []\n",
    "    std = []\n",
    "    nn = [i for i in np.arange(n_min, n_max+1, 1) if i != p]\n",
    "    for n in nn:\n",
    "        temp = []\n",
    "        for t in range(N_TRIALS):\n",
    "\n",
    "            #generate data\n",
    "            X = np.random.normal(size=(n,p))\n",
    "            epsilon = np.random.normal(size=n)\n",
    "            y = np.dot(X, b_star) + epsilon\n",
    "\n",
    "            b_hat = np.dot(np.linalg.pinv(X),y) # find least squares solution\n",
    "\n",
    "            error = np.linalg.norm(b_hat-b_star)**2 # record error\n",
    "            temp.append(error)\n",
    "        errors.append(np.mean(temp))\n",
    "        std.append(np.std(temp))\n",
    "\n",
    "    return np.array(errors), np.array(std)\n",
    "\n",
    "\n",
    "\n",
    "p = 200\n",
    "n_min = 100\n",
    "n_max = 300\n",
    "\n",
    "\n",
    "errors, std = simulation(p=p, n_min=n_min, n_max=n_max)\n",
    "\n",
    "nn = [i for i in np.arange(n_min, n_max+1, 1) if i != p]\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.plot(nn, errors)\n",
    "plt.xlabel('n', fontsize=16)\n",
    "plt.ylabel('Error', fontsize=16)\n",
    "plt.title('Errors as a function of the number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5137b9",
   "metadata": {},
   "source": [
    "Somewhat amazingly, the plot of $e_n$ verses in $n$ is not monotone at all -- in fact, it reaches a maximum at $n=p$! So more data points do not always help us; this is particularly true when $p$ is very large. This phenomenon has recently been referred to as \"double descent\", and it is hypothesized that a similar phenomenon is behind the remarkable recent successes of neural networks."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.12,
    "jupytext_version": "1.9.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "source_map": [
   12,
   30,
   77
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}